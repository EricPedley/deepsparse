# test perplexity script
# test what's going on after we go past sequence_length
# sequences need to be appropriately long to notice the divergence over time


# metric to evaluate logits and cache -> min absolute difference
# maybe if env var set, we can also plot graphs

OUR_MODELS = ["opt", "codegen", "llama"]
for model in OUR_MODELS:

    # establish sources of truth
    torch_target_logits = ...
    ort_target_logits = ...
    torch_target_cache = ...
    ort_target_cache = ...
    
    no_kv_cache_logits = model(kv_cache=False)

    for engine_type in ["onnxruntime", "deepsparse"] 
        ort_no_kv_cache_logits = ...
        deepsparse_no_kv_cache_logits = ...

        if kv_cache:
            for kv_cache_management in ["external", "internal"]

            ort_single_token_prefill_logits = ...
            ort_multi_token_prefill_logits = ...
            ort_single_token_prefill_cache = ...
            ort_multi_token_prefill_cache = ...

            ds_single_token_prefill_logits_external = ...
            ds_multi_token_prefill_logits_external = ...
            ds_single_token_prefill_cache_external = ...
            ds_multi_token_prefill_cache_external = ...

            ds_single_token_prefill_logits_internal = ...
            ds_multi_token_prefill_logits_internal = ...



            
            

        


    
